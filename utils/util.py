import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
import json
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_sample_weight
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

from xgboost import XGBClassifier
from imblearn.ensemble import BalancedRandomForestClassifier
from tabpfn import TabPFNClassifier

def preprocess_data(training_path, features, isplot=True):
    # 1. Caricamento Dataset
    data = pd.read_csv(training_path)
    print(f"Dataset originale: {data.shape}")
    print(f"Feature selezionate: {features}")
    
    # Analisi preliminare sovrapposizione NaN
    analyze_nan_overlap(data, features, isplot=isplot)

    target_col = 'bestclassification_1'
    
    # Lista di colonne fondamentali da controllare (Feature + Target)
    cols_to_check = features + [target_col]
    
    # 2. Analisi dei NaN per Feature
    nan_counts = data[cols_to_check].isna().sum()
    nan_counts = nan_counts[nan_counts > 0].sort_values(ascending=False)
    
    if not nan_counts.empty:
        print("\n--- Riepilogo NaN trovati ---")
        for col, count in nan_counts.items():
            perc = (count / len(data)) * 100
            print(f"Colonna '{col}': {count} mancanti ({perc:.2f}%)")
        
        if isplot:
            plt.figure(figsize=(10, 6))
            sns.barplot(x=nan_counts.values, y=nan_counts.index, palette='viridis')
            plt.title('Conteggio Valori Mancanti (NaN) per Feature')
            plt.xlabel('Numero di NaN')
            plt.ylabel('Feature')
            plt.show()
    else:
        print("\nNessun NaN trovato nelle feature selezionate.")

    # 4. Eliminazione NaN (Drop)
    # Rimuoviamo le righe che hanno ALMENO un NaN nelle colonne selezionate
    initial_rows = data.shape[0]
    data_clean = data.dropna(subset=cols_to_check).copy()
    final_rows = data_clean.shape[0]
    
    print("-" * 30)
    print(f"Righe originali: {initial_rows}")
    print(f"Righe rimaste: {final_rows}")
    print(f"Righe eliminate: {initial_rows - final_rows}")
    print("-" * 30)

    # 5. Creazione X e y (sui dati puliti)
    X = data_clean[features]
    y = data_clean[target_col]

    # === NUOVA SEZIONE: ANALISI DISTRIBUZIONE CLASSI ===
    print("\n--- Distribuzione delle Classi (Dataset Pulito) ---")
    class_counts = y.value_counts().sort_index() # Ordiniamo per etichetta
    total_samples = len(y)
    
    # Stampa testuale
    for cls, count in class_counts.items():
        perc = (count / total_samples) * 100
        print(f"Classe '{cls}': {count} campioni ({perc:.2f}%)")

    if isplot:
        plt.figure(figsize=(10, 6))
        # Usiamo countplot per contare automaticamente le categorie
        # order=class_counts.index assicura che l'ordine sia coerente
        ax = sns.countplot(x=target_col, data=data_clean, palette='magma', order=class_counts.index)
        
        plt.title(f'Distribuzione Classi su {total_samples} campioni totali')
        plt.xlabel('Classe')
        plt.ylabel('Numero di Campioni')
        
        # Aggiunta etichette (Numero e Percentuale) sopra ogni barra
        for p in ax.patches:
            height = p.get_height()
            # Evitiamo errori se height √® 0 o NaN
            if height > 0:
                ax.annotate(f'{int(height)}\n({height/total_samples:.1%})', 
                            (p.get_x() + p.get_width() / 2., height), 
                            ha = 'center', va = 'center', 
                            xytext = (0, 10), 
                            textcoords = 'offset points',
                            fontsize=10, color='black')
        plt.show()
    # ===================================================

    # 6. Encoding del Target
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    print("Mappatura classi:", dict(zip(le.classes_, le.transform(le.classes_))))

    return data_clean, X, y, y_encoded, le

def analyze_nan_overlap(data, features, isplot=True):
    # 1. Filtriamo solo le colonne che hanno effettivamente dei NaN
    cols_with_nan = [col for col in features if data[col].isna().sum() > 0]
    
    if not cols_with_nan:
        # print("Nessun NaN presente nelle feature indicate.")
        return

    # 2. Ordiniamo le colonne dalla pi√π "vuota" (pi√π NaN) alla pi√π piena
    sorted_cols = data[cols_with_nan].isna().sum().sort_values(ascending=False).index.tolist()
    
    print(f"Colonne con NaN (ordinate per quantit√†): {sorted_cols}")

    # 3. CONTROLLO LOGICO DI INCLUSIONE (Subset)
    print("\n--- Verifica Inclusione (Gerarchia) ---")
    
    major_col = sorted_cols[0]
    
    for i in range(1, len(sorted_cols)):
        minor_col = sorted_cols[i]
        
        rows_missing_minor = data[data[minor_col].isna()]
        
        is_subset = rows_missing_minor[major_col].isna().all()
        
        if is_subset:
            print(f"‚úÖ I NaN di '{minor_col}' sono TUTTI inclusi in '{major_col}'")
        else:
            mismatch = (~rows_missing_minor[major_col].isna()).sum()
            print(f"‚ùå '{minor_col}' ha {mismatch} NaN che NON sono presenti in '{major_col}'")
            
    if isplot:
        # 4. VISUALIZZAZIONE (Heatmap)
        plt.figure(figsize=(12, 8))
        sns.heatmap(data[sorted_cols].isna(), cbar=False, cmap='viridis', yticklabels=False)
        plt.title("Mappa dei Valori Mancanti (Giallo = NaN)")
        plt.xlabel("Features (ordinate per numero di NaN)")
        plt.ylabel("Righe del dataset")
        plt.show()

def evaluate_features_and_nans(training_path, target_col='bestclassification_1'):
    # 1. Caricamento Dati Completi (senza filtri iniziali)
    data = pd.read_csv(training_path)
    print(f"Dataset caricato: {data.shape}")
    
    if target_col not in data.columns:
        raise ValueError(f"La colonna target '{target_col}' non esiste nel dataset.")
        
    y = data[target_col]
    X = data.drop(columns=[target_col])
    
    # 3. Preprocessing Automatico per "Far Masticare" tutto al modello
    print("Preprocessing automatico in corso...")
    
    # Identifichiamo le colonne non numeriche (stringhe/oggetti)
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()
    
    print(f"- Feature Numeriche trovate: {len(numeric_cols)}")
    print(f"- Feature Categoriche trovate: {len(categorical_cols)}")
    
    # A. Gestione Stringhe -> Numeri (Ordinal Encoding)
    # Usiamo OrdinalEncoder che gestisce anche i NaN (se configurato) o lo facciamo a mano
    # Per semplicit√† in questa fase di test, convertiamo le stringhe in numeri interi
    if len(categorical_cols) > 0:
        # Creiamo una copia per non toccare i dati originali troppo presto
        X_encoded = X.copy()
        for col in categorical_cols:
            # Verifica rapida: se √® un ID (tutti valori diversi), lo scartiamo
            if X[col].nunique() > len(X) * 0.95:
                print(f"‚ö†Ô∏è Warning: La colonna '{col}' sembra un ID (troppi valori unici). Viene esclusa.")
                X_encoded = X_encoded.drop(columns=[col])
                continue
            
            # Convertiamo stringhe in numeri. pd.factorize gestisce bene i NaN dandogli -1
            X_encoded[col], _ = pd.factorize(X[col])
    else:
        X_encoded = X.copy()

    # Aggiorniamo la lista delle colonne rimaste
    final_features = X_encoded.columns.tolist()

    # 4. Calcolo Percentuale NaN (sui dati originali, per accuratezza)
    # Nota: X_encoded ha trasformato i NaN categorici in -1, ma noi vogliamo sapere quanti erano veri NaN
    nan_percent = (X[final_features].isna().sum() / len(X)) * 100
    
    # 5. Gestione NaN nei Numerici per il modello
    # Sostituiamo i NaN numerici con -999 per permettere al Random Forest di girare
    imputer = SimpleImputer(strategy='constant', fill_value=-999)
    X_final = imputer.fit_transform(X_encoded)
    
    # 6. Preparazione Target (gestione eventuali NaN nel target)
    # Se il target ha NaN, quelle righe sono inutili per il training dell'importanza
    valid_rows = ~y.isna()
    X_final = X_final[valid_rows]
    y = y[valid_rows]
    
    # Encoding del target (stringhe -> numeri) se necessario
    if y.dtype == 'object':
        y_encoded = pd.factorize(y)[0]
    else:
        y_encoded = y

    # 7. Feature Selection con Random Forest
    print("Addestramento modello per calcolo importanza...")
    model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)
    model.fit(X_final, y_encoded)
    
    importances = model.feature_importances_
    
    # 8. Creazione DataFrame Risultati
    analysis_df = pd.DataFrame({
        'Feature': final_features,
        'Importance': importances,
        'NaN_Percent': nan_percent.values
    })
    
    analysis_df = analysis_df.sort_values(by='Importance', ascending=False)
    
    # 9. Output e Grafico
    print("\n--- TOP 35 FEATURES PI√ô IMPORTANTI (SU TUTTO IL DATASET) ---")
    #print(analysis_df.head(35))
    print(analysis_df[analysis_df['NaN_Percent'] < 10.].head(35))
    
    plt.figure(figsize=(14, 10))
    
    # Scatter plot
    sns.scatterplot(
        data=analysis_df, 
        x='Importance', 
        y='NaN_Percent', 
        size='Importance', 
        sizes=(20, 500), 
        hue='Importance', 
        palette='magma',
        legend=False
    )
    
    # Etichette intelligenti: mostriamo solo le feature interessanti
    # (Alta importanza O molti NaN ma importanza media)
    for i in range(analysis_df.shape[0]):
        row = analysis_df.iloc[i]
        # Mostra label se Importanza > soglia media o se √® una top feature
        if row['Importance'] > 0.01: 
            plt.text(
                row['Importance'], 
                row['NaN_Percent'] + 1, # Spostiamo un po' la scritta in su
                row['Feature'], 
                fontsize=9, 
                rotation=15
            )

    plt.title(f"Discovery Mode: Tutte le Feature vs Target '{target_col}'")
    plt.xlabel("Importanza (Random Forest)")
    plt.ylabel("% Valori Mancanti (NaN)")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    return analysis_df

def load_and_process_contaminants(path, features, scaler, max_samples=10000):
    """
    Carica un dataset reale di 'contaminanti'.
    Miglioramenti:
    - Gestione errori di tipo (stringhe al posto di numeri)
    - Rimozione valori infiniti oltre ai NaN
    - Campionamento opzionale se il file √® troppo grosso
    """
    print(f"\n   -> Caricamento dataset contaminanti da: {path}")
    
    # 1. Caricamento
    try:
        # Leggiamo tutto, ma potresti ottimizzare con usecols se il file √® gigantesco
        df_noise = pd.read_csv(path)
    except FileNotFoundError:
        print(f"‚ùå ERRORE: Il file {path} non esiste.")
        return None
    except Exception as e:
        print(f"‚ùå ERRORE durante la lettura del CSV: {e}")
        return None

    # 2. Controllo esistenza feature
    # Set operation √® pi√π veloce e pulita
    missing_cols = set(features) - set(df_noise.columns)
    if missing_cols:
        print(f"‚ùå ERRORE: Nel dataset contaminanti mancano queste colonne: {list(missing_cols)}")
        return None

    # 3. Selezione e Conversione Forzata a Numerico
    # Questo step √® CRUCIALE: trasforma eventuali stringhe sporche in NaN
    X_noise = df_noise[features].copy()
    
    for col in features:
        # errors='coerce' trasforma "errore", "?", " " in NaN
        X_noise[col] = pd.to_numeric(X_noise[col], errors='coerce')

    # 4. Pulizia Approfondita (NaN e Inf)
    initial_len = len(X_noise)
    
    # Sostituiamo infiniti con NaN e poi buttiamo tutto
    X_noise.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_noise = X_noise.dropna()
    
    final_len = len(X_noise)
    diff = initial_len - final_len
    
    if diff > 0:
        print(f"      (Rimossi {diff} campioni sporchi: NaN, stringhe o Inf)")
    
    if final_len == 0:
        print("‚ùå ERRORE: Il dataset contaminanti √® vuoto dopo la pulizia.")
        return None

    # 5. Campionamento (Opzionale)
    # Se hai 1 milione di contaminanti, ne bastano meno per validare il filtro
    if max_samples and final_len > max_samples:
        print(f"      Dataset molto grande ({final_len}). Campionamento a {max_samples} righe...")
        X_noise = X_noise.sample(n=max_samples, random_state=42)

    # 6. Scaling
    # Fondamentale usare .transform() e non .fit_transform()
    try:
        X_noise_scaled = scaler.transform(X_noise)
    except Exception as e:
        print(f"‚ùå ERRORE durante lo scaling dei contaminanti: {e}")
        return None
    
    print(f"      Caricati e pronti {len(X_noise_scaled)} campioni di contaminazione.")
    return X_noise_scaled

def train_isolation_forest(X_train, contamination=0.01):
    print(f"   -> Training Isolation Forest (Contamination={contamination})...")
    iso_forest = IsolationForest(n_estimators=500, contamination=contamination, random_state=42, n_jobs=-1)
    iso_forest.fit(X_train)
    return iso_forest

def train_autoencoder(X_train, encoding_dim=32, epochs=20, batch_size=32):
    print("   -> Training Autoencoder...")
    input_dim = X_train.shape[1]
    
    # Architettura dinamica
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(encoding_dim, activation='relu')(input_layer)
    encoded = Dense(int(encoding_dim/2), activation='relu')(encoded) # Bottleneck
    decoded = Dense(encoding_dim, activation='relu')(encoded)
    output_layer = Dense(input_dim, activation='sigmoid')(decoded) # Output 0-1
    
    autoencoder = Model(input_layer, output_layer)
    autoencoder.compile(optimizer='adam', loss='mse')
    
    autoencoder.fit(
        X_train, X_train,
        epochs=epochs,
        batch_size=batch_size,
        shuffle=True,
        verbose=1,
        validation_split=0.1
    )
    return autoencoder

def get_ae_threshold(autoencoder, X_train, percentile=95):
    reconstructions = autoencoder.predict(X_train, verbose=0)
    mse = np.mean(np.power(X_train - reconstructions, 2), axis=1)
    threshold = np.percentile(mse, percentile)
    return threshold

# ==========================================
# 2. VALUTAZIONE E CONFRONTO FILTRI
# ==========================================

def generate_synthetic_noise(X_sample, n_samples=200):
    input_dim = X_sample.shape[1]
    # Genera rumore random tra -0.2 e 1.2 (per dati scalati 0-1)
    return np.random.uniform(low=-0.2, high=1.2, size=(n_samples, input_dim))

def compare_filters(iso_forest, autoencoder, ae_threshold, X_valid, X_noise):
    print(f"   ‚öîÔ∏è  Confronto Filtri...")

    # 1. Isolation Forest
    # 1 = Inlier (Normale), -1 = Outlier (Anomalia)
    if_valid_preds = iso_forest.predict(X_valid)
    acc_if_valid = np.mean(if_valid_preds == 1) # Vogliamo che siano 1

    if X_noise is not None:
        if_noise_preds = iso_forest.predict(X_noise)
        acc_if_noise = np.mean(if_noise_preds == -1) # Vogliamo che siano -1
    else:
        acc_if_noise = 0.0

    # 2. Autoencoder
    # check_anomaly_ae ora restituisce un array booleano (True=Anomalia)
    ae_test_preds = check_anomaly_ae(autoencoder, X_valid, ae_threshold)
    
    # FIX: Convertiamo esplicitamente in array per sicurezza (anche se la nuova funzione lo fa gi√†)
    ae_test_preds = np.array(ae_test_preds)
    
    # ~ae_test_preds inverte i booleani: True(Anomalia) diventa False.
    # Noi vogliamo contare i NON anomali (Normali) nel validation set.
    acc_ae_valid = np.mean(~ae_test_preds)

    if X_noise is not None:
        ae_noise_preds = check_anomaly_ae(autoencoder, X_noise, ae_threshold)
        ae_noise_preds = np.array(ae_noise_preds)
        # Qui vogliamo che siano anomali (True)
        acc_ae_noise = np.mean(ae_noise_preds)
    else:
        acc_ae_noise = 0.0

    print(f"      IsolationForest -> Valid Acc: {acc_if_valid:.2%}, Noise Rej: {acc_if_noise:.2%}")
    print(f"      Autoencoder     -> Valid Acc: {acc_ae_valid:.2%}, Noise Rej: {acc_ae_noise:.2%}")

    # Logica di scelta: Privilegiamo chi scarta meglio il rumore, 
    # ma deve mantenere almeno il 95% dei dati buoni.
    score_if = acc_if_noise if acc_if_valid > 0.95 else acc_if_noise - 0.5
    score_ae = acc_ae_noise if acc_ae_valid > 0.95 else acc_ae_noise - 0.5
    
    if score_if > score_ae:
        return "IsolationForest"
    else:
        return "Autoencoder"

# ==========================================
# 3. CLASSIFICATORE MULTI-CLASSE
# ==========================================

def train_multiclass_classifier(X_train, y_train, model_type='rf', class_weight='balanced'):
    """
    Addestra il classificatore finale gestendo lo sbilanciamento.
    
    Parametri:
    - model_type: 'rf' per Random Forest, 'xgb' per XGBoost.
    - class_weight: 'balanced' (default) gestisce automaticamente le classi rare.
    """
    
    if model_type == 'rf':
        print(f"\n   -> Training Random Forest (Class Weight: {class_weight})...")
        clf = RandomForestClassifier(
            n_estimators=100, 
            class_weight=class_weight, 
            random_state=42, 
            n_jobs=-1
        )
        clf.fit(X_train, y_train)
        
    elif model_type == 'xgb':
        print(f"\n   -> Training XGBoost (Mode: {model_type})...")
        
        # 1. Calcolo Pesi per gestire lo sbilanciamento (Workaround per XGBoost Multi-class)
        if class_weight == 'balanced':
            print("      (Calcolo sample_weights per bilanciare le classi in XGBoost)")
            sample_weights = compute_sample_weight(
                class_weight='balanced',
                y=y_train
            )
        else:
            sample_weights = None

        # 2. Definizione Modello
        clf = XGBClassifier(
            n_estimators=100,
            max_depth=6,             # Profondit√† standard, aumentala se underfitta
            learning_rate=0.1,       # Standard
            objective='multi:softprob', # Fondamentale per avere le probabilit√†
            num_class=len(np.unique(y_train)),
            n_jobs=-1,
            random_state=42,
            use_label_encoder=False,
            eval_metric='mlogloss'
        )
        
        # 3. Training con pesi
        clf.fit(X_train, y_train, sample_weight=sample_weights)
        
    else:
        raise ValueError(f"Modello '{model_type}' non supportato. Usa 'rf' o 'xgb'.")
        
    return clf

def evaluate_classifier(clf, X_test, y_test, label_encoder=None):
    """
    Stampa classification report e matrice di confusione.
    """
    y_pred = clf.predict(X_test)
    
    if label_encoder:
        target_names = [str(c) for c in label_encoder.classes_]
    else:
        target_names = None
        
    print("\n--- Report Classificazione ---")
    print(classification_report(y_test, y_pred, target_names=target_names))
    
    # Plot Matrice Confusione
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=target_names if target_names else "auto",
                yticklabels=target_names if target_names else "auto")
    plt.title("Matrice di Confusione")
    plt.ylabel('Reale')
    plt.xlabel('Predetto')
    plt.show()

def augment_data_gaussian(X, y, target_count=500, noise_scale=0.02):
    n_present = len(X)
    if n_present >= target_count: return X, y
    n_copies = int(target_count / n_present)
    X_aug_list = [X]; y_aug_list = [y]
    stds = np.std(X, axis=0); stds[stds == 0] = 1.0
    for _ in range(n_copies):
        noise = np.random.normal(0, stds * noise_scale, X.shape)
        X_aug_list.append(X + noise); y_aug_list.append(y)
    return np.vstack(X_aug_list), np.hstack(y_aug_list)

def train_hierarchical_classifier(X_train, y_train, rare_classes=[2, 3, 4], use_augmentation=False):
    """
    Funnel Strategy.
    use_augmentation=False -> Usa i dati puri (~500 Rare vs ~500 Comuni).
    use_augmentation=True  -> Gonfia le rare a 1000 vs 1000 Comuni.
    """
    strategy_name = "Augmented" if use_augmentation else "Pure Data"
    print(f"\n   üèóÔ∏è  Training Funnel ({strategy_name} Strategy)...")
    
    # --- 0. TARGET UNIFICATO ---
    y_merged = y_train.copy()
    mask_rare = np.isin(y_train, rare_classes)
    MERGED_LABEL = 2 
    y_merged[mask_rare] = MERGED_LABEL
    
    # --- 1. GATEKEEPER (BalancedRF) ---
    print(f"      [1/3] Training Gatekeeper...")
    clf_gatekeeper = BalancedRandomForestClassifier(
        n_estimators=300, sampling_strategy="all", 
        replacement=True, random_state=42, n_jobs=-1
    )
    clf_gatekeeper.fit(X_train, y_merged)
    
    # --- 2. SPECIALIST (TabPFN) ---
    X_rare = X_train[mask_rare]
    y_rare = np.full(len(X_rare), MERGED_LABEL)
    
    if use_augmentation:
        # STRATEGIA A: AUGMENTATION (1000 vs 1000)
        print(f"            Augmenting Rare (Target: 1000)...")
        X_rare_final, y_rare_final = augment_data_gaussian(X_rare, y_rare, target_count=1000, noise_scale=0.03)
        n_slots_common = len(y_rare_final) # 1:1 ratio
    else:
        # STRATEGIA B: DATI PURI (120 vs 500)
        # Non tocchiamo le rare. Sono poche ma buone.
        X_rare_final, y_rare_final = X_rare, y_rare
        # Prendiamo un po' pi√π di comuni per coprire la varianza, ma non troppe
        # TabPFN regge bene fino a 1024 totali.
        # Abbiamo ~120 rare. Possiamo mettere ~800 comuni senza problemi.
        n_slots_common = min(800, 1024 - len(y_rare_final))

    # Selezione Comuni (Safety Net)
    mask_common = ~mask_rare
    X_common_all = X_train[mask_common]
    
    idx_common = np.random.choice(len(X_common_all), size=n_slots_common, replace=False)
    X_bg = X_common_all[idx_common]
    y_bg = np.full(n_slots_common, 99) # 99 = Falso Allarme
    
    # Unione
    X_spec = np.vstack([X_rare_final, X_bg])
    y_spec = np.hstack([y_rare_final, y_bg])
    
    print(f"      [2/3] Training Specialist (TabPFN): {len(X_spec)} samples.")
    print(f"            ({len(y_rare_final)} Rare Reali vs {len(y_bg)} Comuni Random)")
    
    clf_specialist = TabPFNClassifier(device='auto', n_estimators=32)
    clf_specialist.fit(X_spec, y_spec)
    
    # --- 3. GENERALIST (XGBoost) ---
    print(f"      [3/3] Training Generalist...")
    y_common_labels = y_train[mask_common]
    sample_weights = compute_sample_weight(class_weight='balanced', y=y_common_labels)
    
    clf_generalist = XGBClassifier(
        n_estimators=200, max_depth=6, learning_rate=0.1,
        n_jobs=-1, random_state=42, eval_metric='mlogloss'
    )
    clf_generalist.fit(X_common_all, y_common_labels, sample_weight=sample_weights)
    
    return {
        "gatekeeper": clf_gatekeeper,
        "specialist": clf_specialist,
        "generalist": clf_generalist
    }

def analyze_pipeline_components(X, y_true, models_dict, gate_thresh=0.05, spec_thresh=0.90):
    print(f"\n{'#'*60}")
    print(f"üî¨ DIAGNOSTICA COMPONENTI PIPELINE")
    print(f"{'#'*60}")
    
    # Mappiamo y_true a 3 classi: 0, 1, 2 (Rare unificate)
    y_merged = y_true.copy()
    mask_rare = np.isin(y_true, [2, 3, 4])
    y_merged[mask_rare] = 2
    
    # ---------------------------------------------------------
    # 1. ANALISI GATEKEEPER (Il setaccio grosso)
    # ---------------------------------------------------------
    print(f"\n1Ô∏è‚É£  ANALISI GATEKEEPER (Soglia {gate_thresh})")
    clf_gate = models_dict["gatekeeper"]
    
    # Probabilit√† che sia classe 2 (Rara)
    gate_probs = clf_gate.predict_proba(X)
    idx_rare = np.where(clf_gate.classes_ == 2)[0][0]
    prob_is_rare = gate_probs[:, idx_rare]
    
    # Chi passa il cancello?
    gate_preds = (prob_is_rare >= gate_thresh).astype(int) # 1 = Passa (Sospetto Raro), 0 = Bloccato
    
    # Target vero binario per il Gatekeeper (1 = √à Raro, 0 = √à Comune)
    y_gate_target = (y_merged == 2).astype(int)
    
    print(classification_report(y_gate_target, gate_preds, target_names=["Bloccato (Comune)", "Passa (Sospetto)"]))
    
    cm = confusion_matrix(y_gate_target, gate_preds)
    tn, fp, fn, tp = cm.ravel()
    print(f"   -> Rari Persi (FN): {fn} (GRAVE!)")
    print(f"   -> Rari Trovati (TP): {tp}")
    print(f"   -> Falsi Allarmi passati allo Specialist (FP): {fp}")

    # ---------------------------------------------------------
    # 2. ANALISI SPECIALIST (Il setaccio fine)
    # ---------------------------------------------------------
    print(f"\n2Ô∏è‚É£  ANALISI SPECIALIST (TabPFN - Soglia {spec_thresh})")
    
    # Analizziamo SOLO ci√≤ che il Gatekeeper ha lasciato passare
    mask_passed = (gate_preds == 1)
    
    if np.sum(mask_passed) > 0:
        X_spec = X[mask_passed]
        y_spec_true = y_merged[mask_passed] # Label vere (0, 1, 2)
        
        clf_spec = models_dict["specialist"]
        spec_probs_all = clf_spec.predict_proba(X_spec)
        spec_probs = np.max(spec_probs_all, axis=1) # Confidenza massima
        spec_preds_raw = clf_spec.predict(X_spec)
        
        # Logica Soglia Specialist
        # Se predice 2 (Raro) MA confidenza < soglia -> diventa 99 (Comune)
        mask_uncertain = (spec_preds_raw == 2) & (spec_probs < spec_thresh)
        spec_preds_final = spec_preds_raw.copy()
        spec_preds_final[mask_uncertain] = 99
        
        # Per il report: 1 = Confermato Raro, 0 = Scartato come Comune
        spec_binary_pred = (spec_preds_final == 2).astype(int)
        
        # Target vero: Era davvero raro?
        y_spec_target = (y_spec_true == 2).astype(int)
        
        print(f"   Campioni analizzati: {len(X_spec)} (di cui {np.sum(y_spec_target)} rari veri)")
        print(classification_report(y_spec_target, spec_binary_pred, target_names=["Scartato (Falso Allarme)", "Confermato Raro"]))
        
        cm = confusion_matrix(y_spec_target, spec_binary_pred)
        tn, fp, fn, tp = cm.ravel()
        print(f"   -> Rari Salvati (TP): {tp}")
        print(f"   -> Rari Persi per troppa severit√† (FN): {fn}")
        print(f"   -> Falsi Allarmi Ripuliti (TN): {tn} (Bravo TabPFN!)")
        print(f"   -> Falsi Allarmi Rimasti (FP): {fp}")
    else:
        print("   Nessun campione ha passato il Gatekeeper.")

    # ---------------------------------------------------------
    # 3. ANALISI GENERALIST (0 vs 1)
    # ---------------------------------------------------------
    print(f"\n3Ô∏è‚É£  ANALISI GENERALIST (XGBoost 0 vs 1)")
    
    # Il generalista vede:
    # A. Quelli bloccati dal Gatekeeper
    # B. Quelli scartati dallo Specialist
    # Per semplicit√†, valutiamolo su TUTTE le stelle comuni vere (0 e 1)
    # per vedere quanto √® bravo nel suo lavoro base.
    
    mask_common_true = (y_merged != 2)
    X_gen = X[mask_common_true]
    y_gen_true = y_merged[mask_common_true]
    
    clf_gen = models_dict["generalist"]
    gen_preds = clf_gen.predict(X_gen)
    
    print(classification_report(y_gen_true, gen_preds, target_names=["RRab", "RRc"]))

def analyze_contaminants_flow(X_noise, filter_model, filter_type, ae_threshold, models_dict, gate_thresh=0.05, spec_thresh=0.90):
    print(f"\n{'#'*60}")
    print(f"üëΩ ANALISI FLUSSO CONTAMINANTI ({len(X_noise)} campioni)")
    print(f"{'#'*60}")
    
    n_total = len(X_noise)
    
    # ---------------------------------------------------------
    # 1. LIVELLO FILTRO (Anomaly Detection)
    # ---------------------------------------------------------
    print(f"\n1Ô∏è‚É£  LIVELLO FILTRO ({filter_type})")
    
    if filter_type == 'IsolationForest':
        # -1 = Anomalia (Bloccato), 1 = Normale (Passa)
        filter_preds = filter_model.predict(X_noise)
        n_blocked = np.sum(filter_preds == -1)
        mask_passed = (filter_preds == 1)
    else:
        # Autoencoder
        is_anomaly = check_anomaly_ae(filter_model, X_noise, ae_threshold)
        n_blocked = np.sum(is_anomaly)
        mask_passed = ~is_anomaly
        
    n_passed = np.sum(mask_passed)
    print(f"   -> Bloccati: {n_blocked} ({n_blocked/n_total:.1%}) ‚úÖ")
    print(f"   -> Passati:  {n_passed} ({n_passed/n_total:.1%}) ‚ö†Ô∏è")
    
    if n_passed == 0:
        print("\n   üéâ OTTIMO! Il filtro ha bloccato tutto. Nessun contaminante entra nella pipeline.")
        return

    # Prendiamo solo i sopravvissuti
    X_survivors = X_noise[mask_passed]
    
    # ---------------------------------------------------------
    # 2. LIVELLO GATEKEEPER (Balanced RF)
    # ---------------------------------------------------------
    print(f"\n2Ô∏è‚É£  LIVELLO GATEKEEPER (Soglia {gate_thresh})")
    clf_gate = models_dict["gatekeeper"]
    
    # Probabilit√† Raro (Classe 2)
    gate_probs = clf_gate.predict_proba(X_survivors)
    # Troviamo indice classe 2
    classes = clf_gate.classes_
    try:
        idx_rare = np.where(classes == 2)[0][0]
        prob_is_rare = gate_probs[:, idx_rare]
    except:
        prob_is_rare = np.zeros(len(X_survivors))
        
    # Maschera: Chi va dallo specialista?
    mask_to_spec = (prob_is_rare >= gate_thresh)
    n_to_spec = np.sum(mask_to_spec)
    n_to_gen = len(X_survivors) - n_to_spec
    
    print(f"   -> Mandati al Generalist (0/1): {n_to_gen} ({n_to_gen/n_total:.1%})")
    print(f"   -> Mandati allo Specialist (Sospetti): {n_to_spec} ({n_to_spec/n_total:.1%}) üö®")
    
    if n_to_spec == 0:
        print("   Nessun contaminante √® stato ritenuto 'Sospetto Raro'.")
        return

    # Prendiamo i sospetti
    X_suspects = X_survivors[mask_to_spec]

    # ---------------------------------------------------------
    # 3. LIVELLO SPECIALIST (TabPFN)
    # ---------------------------------------------------------
    print(f"\n3Ô∏è‚É£  LIVELLO SPECIALIST (TabPFN - Soglia {spec_thresh})")
    clf_spec = models_dict["specialist"]
    
    spec_preds = clf_spec.predict(X_suspects)
    spec_probs_all = clf_spec.predict_proba(X_suspects)
    spec_probs = np.max(spec_probs_all, axis=1)
    
    # Logica finale: √à confermato Raro (2) SOLO SE prob >= soglia
    # Altrimenti viene declassato a Falso Allarme (99)
    mask_confirmed_rare = (spec_preds == 2) & (spec_probs >= spec_thresh)
    
    n_leaked = np.sum(mask_confirmed_rare)
    n_saved = len(X_suspects) - n_leaked
    
    print(f"   -> Riconosciuti come Falsi Allarmi: {n_saved} ‚úÖ")
    print(f"   -> ERRONEAMENTE CLASSIFICATI 'RARI': {n_leaked} ({n_leaked/n_total:.1%}) ‚ùå")
    
    if n_leaked > 0:
        print(f"   ‚ö†Ô∏è ATTENZIONE: {n_leaked} contaminanti sono finiti nel catalogo finale delle stelle rare!")
    else:
        print(f"   üèÜ SUCCESSO: TabPFN ha ripulito tutti i contaminanti residui!")

def analyze_contaminants_statistics(path, features, label_col=None):
    """
    Esegue un'analisi esplorativa sui contaminanti simile a quella fatta per il training set.
    - Gerarchia dei NaN
    - Conteggio righe perse
    - Distribuzione delle tipologie di contaminanti (se label_col √® presente)
    """
    print(f"\n{'='*60}")
    print(f"üïµÔ∏è  ANALISI STATISTICA DATASET CONTAMINANTI")
    print(f"{'='*60}")
    print(f"File: {path}")

    # 1. Caricamento
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        print(f"‚ùå Errore: File non trovato.")
        return
    
    # Verifica colonne
    missing_cols = [c for c in features if c not in df.columns]
    if missing_cols:
        print(f"‚ö†Ô∏è Attenzione: Mancano le colonne: {missing_cols}")
        return

    # 2. Analisi NaN Gerarchica
    # Filtriamo solo le colonne features che hanno NaN
    cols_with_nan = [col for col in features if df[col].isna().sum() > 0]
    
    if not cols_with_nan:
        print("\n‚úÖ Nessun NaN trovato nelle features dei contaminanti.")
    else:
        # Ordiniamo per quantit√† di NaN
        nan_counts = df[cols_with_nan].isna().sum().sort_values(ascending=False)
        sorted_cols = nan_counts.index.tolist()
        
        print(f"\nColonne con NaN (ordinate): {sorted_cols}")
        print("\n--- Verifica Inclusione (Gerarchia) ---")
        
        major_col = sorted_cols[0] # La colonna con pi√π NaN
        
        for i in range(1, len(sorted_cols)):
            minor_col = sorted_cols[i]
            
            # Se minor_col √® NaN, major_col √® anch'essa NaN?
            rows_missing_minor = df[df[minor_col].isna()]
            is_subset = rows_missing_minor[major_col].isna().all()
            
            if is_subset:
                print(f"‚úÖ I NaN di '{minor_col}' sono TUTTI inclusi in '{major_col}'")
            else:
                mismatch = (~rows_missing_minor[major_col].isna()).sum()
                print(f"‚ùå '{minor_col}' ha {mismatch} NaN che NON sono presenti in '{major_col}'")

        print("\n--- Riepilogo NaN Contaminanti ---")
        for col, count in nan_counts.items():
            perc = (count / len(df)) * 100
            print(f"Colonna '{col}': {count} mancanti ({perc:.2f}%)")

    # 3. Pulizia e Conteggio Righe
    initial_rows = len(df)
    # Puliamo basandoci solo sulle features usate dal modello
    df_clean = df.dropna(subset=features)
    final_rows = len(df_clean)
    removed = initial_rows - final_rows
    
    print("-" * 30)
    print(f"Righe originali: {initial_rows}")
    print(f"Righe rimaste (Pulite): {final_rows}")
    print(f"Righe eliminate: {removed} ({(removed/initial_rows)*100:.1f}%)")
    print("-" * 30)

    # 4. Distribuzione Tipi Contaminanti (Opzionale)
    # Se il file ha una colonna tipo 'label', 'type', 'class' che dice cos'√® l'oggetto
    if label_col:
        if label_col in df.columns:
            print(f"\n--- Distribuzione Tipi Contaminanti ({label_col}) ---")
            counts = df_clean[label_col].value_counts()
            for cls, count in counts.items():
                perc = (count / final_rows) * 100
                print(f"Tipo '{cls}': {count} campioni ({perc:.2f}%)")
        else:
            print(f"\n‚ö†Ô∏è Colonna label '{label_col}' non trovata nel CSV.")
    else:
        # Se non specifichi la colonna label, proviamo a indovinarne alcune comuni
        possible_labels = ['label', 'class', 'type', 'bestclassification', 'bestclassification_1', 'source_type']
        found = [c for c in possible_labels if c in df.columns]
        if found:
            print(f"\n--- Distribuzione Tipi (Colonna indovinata: '{found[0]}') ---")
            print(df_clean[found[0]].value_counts().head(10)) # Top 10 tipi

# ==========================================
# 4. GESTIONE I/O (SALVATAGGIO/CARICAMENTO)
# ==========================================

def save_hierarchical_system(save_dir, exp_name, scaler, filter_model, filter_type, models_dict, features, rare_classes, ae_thresh=None, le=None):
    """
    Salva l'intero ecosistema di modelli in una cartella specifica.
    """
    import json
    path = os.path.join(save_dir, exp_name)
    os.makedirs(path, exist_ok=True)
    
    print(f"üíæ Salvataggio sistema in: {path}")
    
    # 1. Salva Scaler e LabelEncoder
    joblib.dump(scaler, os.path.join(path, "scaler.joblib"))
    if le:
        joblib.dump(le, os.path.join(path, "le.joblib"))
    
    # 2. Salva il Filtro (Anomaly Detector)
    if filter_type == "Autoencoder":
        # I modelli Keras si salvano in formato .h5 o .keras
        filter_model.save(os.path.join(path, "filter_ae.h5"))
    else:
        joblib.dump(filter_model, os.path.join(path, "filter_if.joblib"))
        
    # 3. Salva i componenti del Funnel
    joblib.dump(models_dict['gatekeeper'], os.path.join(path, "gatekeeper.joblib"))
    joblib.dump(models_dict['specialist'], os.path.join(path, "specialist.joblib"))
    joblib.dump(models_dict['generalist'], os.path.join(path, "generalist.joblib"))
    
    # 4. Salva i Metadati (Features e Soglie)
    metadata = {
        "features": features,
        "rare_classes": [int(c) for c in rare_classes],
        "filter_type": filter_type,
        "ae_threshold": float(ae_thresh) if ae_thresh is not None else None,
        "exp_name": exp_name
    }
    with open(os.path.join(path, "metadata.json"), "w") as f:
        json.dump(metadata, f, indent=4)
        
    print("‚úÖ Salvataggio completato!")

def load_hierarchical_system(model_dir, exp_name):
    """
    Caricamento standard.
    Richiede che l'ambiente abbia GPU se i modelli sono stati salvati su GPU.
    """
    path = os.path.join(model_dir, exp_name)
    if not os.path.exists(path):
        raise FileNotFoundError(f"Cartella non trovata: {path}")

    print(f"üìÇ Caricamento modelli da: {path}")

    # 1. Metadata
    meta_path = os.path.join(path, "metadata.json")
    with open(meta_path, 'r') as f:
        metadata = json.load(f)

    # 2. Scaler & LabelEncoder
    scaler = joblib.load(os.path.join(path, "scaler.joblib"))
    le = joblib.load(os.path.join(path, "le.joblib"))

    # 3. Filtro (Keras Autoencoder o IsolationForest)
    filter_type = metadata.get('filter_type', 'IsolationForest')
    filter_model = None
    
    if filter_type == 'Autoencoder':
        # Cerca prima .h5 (pi√π stabile), poi .keras
        ae_path_h5 = os.path.join(path, "filter_ae.h5")
        ae_path_keras = os.path.join(path, "filter_ae.keras")
        
        target_path = ae_path_h5 if os.path.exists(ae_path_h5) else ae_path_keras
        
        try:
            print(f"   üîå Caricamento Keras da: {os.path.basename(target_path)}")
            filter_model = tf.keras.models.load_model(target_path, compile=False)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Fallito caricamento standard, riprovo con custom objects: {e}")
            filter_model = tf.keras.models.load_model(
                target_path, 
                custom_objects={'mse': tf.keras.losses.MeanSquaredError()}
            )
    else:
        filter_model = joblib.load(os.path.join(path, "filter_if.joblib"))

    # 4. Classificatori Funnel (Joblib Standard)
    models_dict = {}
    try:
        print("   ‚öôÔ∏è  Caricamento modelli TabPFN (Joblib standard)...")
        # Joblib gestir√† internamente torch se necessario, usando la GPU disponibile
        models_dict['gatekeeper'] = joblib.load(os.path.join(path, "gatekeeper.joblib"))
        models_dict['specialist'] = joblib.load(os.path.join(path, "specialist.joblib"))
        models_dict['generalist'] = joblib.load(os.path.join(path, "generalist.joblib"))
    except Exception as e:
        raise RuntimeError(f"‚ùå Errore caricamento modelli Funnel: {e}")

    return {
        "scaler": scaler,
        "le": le,
        "filter_model": filter_model,
        "filter_type": filter_type,
        "ae_threshold": metadata.get('ae_threshold', None),
        "features": metadata['features'],
        "rare_classes": metadata.get('rare_classes', []),
        "models_dict": models_dict
    }
def predict_hierarchical_batch(X_scaled, models_dict, gate_threshold=0.05, spec_threshold=0.90):
    """
    VERSIONE TOTALMENTE VETTORIZZATA PER GPU.
    Processa migliaia di righe contemporaneamente.
    """
    clf_gate = models_dict["gatekeeper"]
    clf_spec = models_dict["specialist"]
    clf_gen  = models_dict["generalist"]
    
    n_samples = X_scaled.shape[0]
    final_preds = np.zeros(n_samples, dtype=int)
    final_probs = np.zeros(n_samples, dtype=float)
    status_flags = np.full(n_samples, "Generalist_Direct", dtype=object)

    # 1. GATEKEEPER (CPU - RF √® veloce in batch)
    gate_probs_all = clf_gate.predict_proba(X_scaled)
    # Assumiamo che la classe '2' (Rare) sia all'indice 1 del Gatekeeper
    # Se il gatekeeper √® binario (Comune vs Raro), l'indice 1 √® il sospetto raro
    prob_is_rare = gate_probs_all[:, 1] 

    mask_to_specialist = (prob_is_rare >= gate_threshold)
    mask_to_generalist = ~mask_to_specialist

    # 2. GENERALIST (Batch su tutto ci√≤ che non √® sospetto raro)
    if np.any(mask_to_generalist):
        X_gen = X_scaled[mask_to_generalist]
        final_preds[mask_to_generalist] = clf_gen.predict(X_gen)
        final_probs[mask_to_generalist] = np.max(clf_gen.predict_proba(X_gen), axis=1)

    # 3. SPECIALIST (TabPFN su GPU - Qui avviene la magia)
    if np.any(mask_to_specialist):
        X_spec = X_scaled[mask_to_specialist]
        
        # TabPFN riconosce automaticamente la GPU se disponibile
        spec_preds_raw = clf_spec.predict(X_spec)
        spec_probs_all = clf_spec.predict_proba(X_spec)
        spec_probs = np.max(spec_probs_all, axis=1)
        
        # Logica di raffinamento (Confidence Check)
        # Se TabPFN predice una rara (es. label 2, 3, 4) ma con bassa confidenza
        # lo rimandiamo al generalista
        is_predicted_rare = np.isin(spec_preds_raw, [2, 3, 4])
        mask_low_conf = is_predicted_rare & (spec_probs < spec_threshold)
        
        # Quelli confermati rari
        mask_confirmed = is_predicted_rare & ~mask_low_conf
        
        # Prepariamo i risultati dello specialist
        temp_preds = spec_preds_raw.copy()
        temp_status = np.full(len(X_spec), "Confirmed_Rare", dtype=object)
        
        # Quelli che TabPFN scarta o sono incerti vanno al Generalista
        mask_reject = ~mask_confirmed
        if np.any(mask_reject):
            X_retry = X_spec[mask_reject]
            temp_preds[mask_reject] = clf_gen.predict(X_retry)
            spec_probs[mask_reject] = np.max(clf_gen.predict_proba(X_retry), axis=1)
            temp_status[mask_reject] = "Rejected_by_Spec"

        final_preds[mask_to_specialist] = temp_preds
        final_probs[mask_to_specialist] = spec_probs
        status_flags[mask_to_specialist] = temp_status

    return final_preds, final_probs, status_flags

def predict_hierarchical_single(X_sample, models_dict, gate_threshold=0.05, spec_threshold=0.90):
    """
    VERSIONE PER SINGOLO CAMPIONE (INFERENZA)
    Restituisce un dizionario.
    """
    if X_sample.ndim == 1:
        X_sample = X_sample.reshape(1, -1)
    
    # Usa la versione batch internamente per coerenza
    preds, confs, flags = predict_hierarchical_batch(X_sample, models_dict, 
                                                    gate_threshold=gate_threshold, 
                                                    spec_threshold=spec_threshold)
    
    return {
        "class": preds[0],
        "confidence": confs[0],
        "stage": flags[0]
    }

def check_anomaly_ae(autoencoder, X_sample, threshold, return_mse=False):
    """
    Versione corretta che gestisce sia singole righe che interi batch (dataset completi).
    Restituisce NumPy Array (non liste), risolvendo l'errore del ~
    """
    # Assicuriamo che l'input sia 2D
    if X_sample.ndim == 1:
        X_sample = X_sample.reshape(1, -1)

    # Predizione (batch)
    reconstructed = autoencoder.predict(X_sample, verbose=0)
    
    # Calcolo MSE per ogni riga (axis=1)
    # NOTA: Rimosso [0] che c'era nelle versioni precedenti per supportare batch size > 1
    mse = np.mean(np.power(X_sample - reconstructed, 2), axis=1)
    
    # Confronto vettorizzato
    is_anomaly = mse > threshold
    
    if return_mse:
        return is_anomaly, mse
        
    return is_anomaly  # Restituisce un numpy array di booleani

def compare_filters(iso_forest, autoencoder, ae_threshold, X_valid, X_noise):
    print(f"   ‚öîÔ∏è  Confronto Filtri...")

    # 1. Isolation Forest
    # 1 = Inlier (Normale), -1 = Outlier (Anomalia)
    if_valid_preds = iso_forest.predict(X_valid)
    acc_if_valid = np.mean(if_valid_preds == 1) # Vogliamo che siano 1

    if X_noise is not None:
        if_noise_preds = iso_forest.predict(X_noise)
        acc_if_noise = np.mean(if_noise_preds == -1) # Vogliamo che siano -1
    else:
        acc_if_noise = 0.0

    # 2. Autoencoder
    # check_anomaly_ae ora restituisce un array booleano (True=Anomalia)
    ae_test_preds = check_anomaly_ae(autoencoder, X_valid, ae_threshold)
    
    # FIX: Convertiamo esplicitamente in array per sicurezza (anche se la nuova funzione lo fa gi√†)
    ae_test_preds = np.array(ae_test_preds)
    
    # ~ae_test_preds inverte i booleani: True(Anomalia) diventa False.
    # Noi vogliamo contare i NON anomali (Normali) nel validation set.
    acc_ae_valid = np.mean(~ae_test_preds)

    if X_noise is not None:
        ae_noise_preds = check_anomaly_ae(autoencoder, X_noise, ae_threshold)
        ae_noise_preds = np.array(ae_noise_preds)
        # Qui vogliamo che siano anomali (True)
        acc_ae_noise = np.mean(ae_noise_preds)
    else:
        acc_ae_noise = 0.0

    print(f"      IsolationForest -> Valid Acc: {acc_if_valid:.2%}, Noise Rej: {acc_if_noise:.2%}")
    print(f"      Autoencoder     -> Valid Acc: {acc_ae_valid:.2%}, Noise Rej: {acc_ae_noise:.2%}")

    # Logica di scelta: Privilegiamo chi scarta meglio il rumore, 
    # ma deve mantenere almeno il 95% dei dati buoni.
    score_if = acc_if_noise if acc_if_valid > 0.95 else acc_if_noise - 0.5
    score_ae = acc_ae_noise if acc_ae_valid > 0.95 else acc_ae_noise - 0.5
    
    if score_if > score_ae:
        return "IsolationForest"
    else:
        return "Autoencoder"


def check_anomaly_ae(autoencoder, X_sample, threshold, return_mse=False):
    """
    Versione corretta che gestisce sia singole righe che interi batch (dataset completi).
    Restituisce NumPy Array (non liste), risolvendo l'errore del ~
    """
    # Assicuriamo che l'input sia 2D
    if X_sample.ndim == 1:
        X_sample = X_sample.reshape(1, -1)

    # Predizione (batch)
    reconstructed = autoencoder.predict(X_sample, verbose=0)
    
    # Calcolo MSE per ogni riga (axis=1)
    # NOTA: Rimosso [0] che c'era nelle versioni precedenti per supportare batch size > 1
    mse = np.mean(np.power(X_sample - reconstructed, 2), axis=1)
    
    # Confronto vettorizzato
    is_anomaly = mse > threshold
    
    if return_mse:
        return is_anomaly, mse
        
    return is_anomaly  # Restituisce un numpy array di booleani

def full_pipeline_predict(raw_data, system, threshold=0.90): 
    """Wrapper Imbuto."""
    scaler = system['scaler']
    data_scaled = scaler.transform(raw_data.reshape(1, -1))
    
    # 1. Filtro
    is_unknown = False
    if system['filter_type'] == 'IsolationForest':
        if system['filter_model'].predict(data_scaled)[0] == -1: is_unknown = True
    else:
        if check_anomaly_ae(system['filter_model'], data_scaled, system['ae_threshold'])[0]: is_unknown = True
            
    if is_unknown: return "SCONOSCIUTO (Filtro)"
    
    # 2. Imbuto
    res = predict_hierarchical_single(
        data_scaled, 
        system['models_dict'],
        spec_threshold=threshold
    )
    
    pred_class = int(res["class"])
    confidence = float(res["confidence"])
    status = res["stage"]

    label_str = str(pred_class)
    if 'le' in system and system['le']:
        try: label_str = system['le'].inverse_transform([pred_class])[0]
        except: pass

    base_msg = f"Classe {label_str} (Conf: {confidence:.2f})"
    
    if status == "Confirmed_Rare":
        return f"üíé RARA CONFERMATA: {base_msg}"
        
    if status == "Rejected_by_Spec":
        return f"‚ö†Ô∏è SOSPETTO (Scartato da Specialist): {base_msg}"
        
    return base_msg

import pandas as pd
import numpy as np

def filter_known_sources(df_target, train_path, contaminants_path, id_col='sourceid'):
    """
    Rimuove dal dataframe target le righe che compaiono nel training set 
    o nel file dei contaminanti, basandosi sulla colonna ID.
    
    Args:
        df_target (pd.DataFrame): Il dataframe da pulire (evaluation set).
        train_path (str): Path del file CSV di training.
        contaminants_path (str): Path del file CSV dei contaminanti.
        id_col (str): Nome della colonna identificativa (es. 'sourceid').
        
    Returns:
        pd.DataFrame: Il dataframe senza le sorgenti note.
        dict: Statistiche sulla rimozione.
    """
    print(f"\nüßπ FILTRO SORGENTI NOTE (Training & Contaminants)...")
    initial_len = len(df_target)
    
    # Assicuriamoci che la colonna ID esista nel target
    if id_col not in df_target.columns:
        print(f"‚ö†Ô∏è Attenzione: Colonna '{id_col}' non trovata nel dataset target. Skip filtro overlap.")
        return df_target, {"removed_train": 0, "removed_cont": 0}

    # Set degli ID da rimuovere (usiamo stringhe per evitare problemi int vs str)
    ids_to_remove = set()
    stats = {"removed_train": 0, "removed_cont": 0}

    # 1. Carica Training IDs
    if train_path and os.path.exists(train_path):
        try:
            # Leggiamo solo la colonna ID per efficienza
            df_train = pd.read_csv(train_path, usecols=[id_col])
            train_ids = set(df_train[id_col].astype(str))
            ids_to_remove.update(train_ids)
            stats["removed_train"] = len(train_ids)
            print(f"   Training Set IDs caricati: {len(train_ids)}")
        except Exception as e:
            print(f"‚ö†Ô∏è Errore lettura Training set: {e}")

    # 2. Carica Contaminants IDs
    if contaminants_path and os.path.exists(contaminants_path):
        try:
            df_cont = pd.read_csv(contaminants_path, usecols=[id_col])
            cont_ids = set(df_cont[id_col].astype(str))
            ids_to_remove.update(cont_ids)
            stats["removed_cont"] = len(cont_ids)
            print(f"   Contaminants IDs caricati: {len(cont_ids)}")
        except Exception as e:
            # Alcuni file contaminanti potrebbero non avere header o avere nomi diversi
            print(f"‚ö†Ô∏è Errore lettura Contaminants set: {e}")

    # 3. Filtro effettivo
    # Convertiamo gli ID del target in stringa per il confronto
    mask_overlap = df_target[id_col].astype(str).isin(ids_to_remove)
    df_clean = df_target[~mask_overlap].copy()
    
    removed_count = mask_overlap.sum()
    print(f"   Sorgenti rimosse (gi√† note): {removed_count}")
    print(f"   Dataset residuo: {len(df_clean)}")
    
    return df_clean, stats


def analyze_data_quality(df, features):
    """
    Analizza la presenza di NaN e valori Infiniti nelle features specificate.
    Restituisce un report e la maschera delle righe valide.
    
    Args:
        df (pd.DataFrame): Il dataframe da analizzare.
        features (list): Lista delle features da controllare.
        
    Returns:
        pd.Series: Maschera booleana (True = riga valida).
        pd.DataFrame: Report delle features mancanti/errate.
    """
    print(f"\nüìä ANALISI QUALIT√Ä DATI (Features: {len(features)})...")
    
    # Verifica esistenza colonne
    missing_cols = [c for c in features if c not in df.columns]
    if missing_cols:
        print(f"‚ùå ERRORE CRITICO: Mancano le colonne: {missing_cols}")
        return pd.Series([False]*len(df), index=df.index), None

    subset = df[features]
    
    # 1. Check NaN
    nans = subset.isna().sum()
    
    # 2. Check Inf
    infs = ((subset == np.inf) | (subset == -np.inf)).sum()
    
    # 3. Totale righe "sporche" per colonna
    bad_data = nans + infs
    
    # Creiamo un report
    report = pd.DataFrame({
        'NaNs': nans,
        'Infs': infs,
        'Total Bad': bad_data,
        'Percent Bad': (bad_data / len(df) * 100).round(2)
    })
    
    # Filtriamo solo quelle che hanno problemi per visualizzazione pulita
    problematic_features = report[report['Total Bad'] > 0]
    
    if not problematic_features.empty:
        print("   Features con problemi (NaN o Inf):")
        print(problematic_features.sort_values(by='Total Bad', ascending=False))
    else:
        print("   ‚úÖ Nessun NaN o Inf trovato nelle features!")

    # 4. Creazione Maschera Righe Valide (tutte le features ok)
    # Sostituiamo inf con nan temporaneamente per fare un check unico
    temp_subset = subset.replace([np.inf, -np.inf], np.nan)
    valid_rows_mask = temp_subset.notna().all(axis=1)
    
    print(f"   Righe completamente valide: {valid_rows_mask.sum()} / {len(df)} ({valid_rows_mask.mean():.1%})")
    
    return valid_rows_mask, report